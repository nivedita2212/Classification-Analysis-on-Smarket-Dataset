---
title: "classification"
author: "Nivedita"
date: "2024-04-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

To perform logistic regression, linear discriminant analysis, naive bayes, quadratic discriminant analysis and k nearest neighbors on Smarket dataset in ISLR2 library.

## Analysis

Smarket data set consists of percent age returns for the S&P 500 stock index over1, 250days, from the beginning of 2001 until the end of 2005. For each date, it has recorded the percentage returns for each of the five previous trading days, Lag1 through Lag5 and also recorded Volume (the number of shares traded on the previous day, in billions), Today(the percentage return on the date in question) and Direction (whether the market was Up or Down on this date). Our goal is to predict Direction (a qualitative response) using the other features.

```{r echo=TRUE}
library(ISLR2)
names(Smarket)
dim(Smarket)
```

We will perform the analysis on the Smarket data, which is part of the ISLR2 library. 

```{r echo=TRUE}
cor(Smarket[,-9])
```

We can observe that the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and Volume. 

```{r echo=TRUE}
attach(Smarket)
plot(Year,Volume,main='Number of shares traded on the previous day in 2001-2005')
```

*Fig. 1 Plot of volume over the period of time*

From Fig. 1 we can see that Volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.

#### Logistic Regression

We will fit a logistic regression model in order to predict Direction using Lag 1 through Lag5 and Volume.

```{r echo= TRUE}
#Logistic regression
glm.fits <-glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,data = Smarket, family = binomial)
summary(glm.fits)
```

We can see that the smallest p-value here is associated with Lag1. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to goup today. However, at a value of 0.15, the p-value is still relatively large, and so there is no clear evidence of a real association between Lag1 and Direction.


In order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than 0.5.

```{r echo=TRUE}
glm.probs <- predict(glm.fits, type = "response")
glm.pred <- rep("Down", 1250)
glm.pred[glm.probs > .5] = "Up"
```

We can create confusion matrix in order to determine how many observations were correctly or incorrectly classified.

```{r echo=TRUE}
table(glm.pred, Direction)
```
```{r echo=TRUE}
(507 + 145) / 1250
```

we can see that the model correctly predicted that the market would go up on 507 days and that it would go down on 145 days. In this case, logistic regression correctly predicted the movement of the market 52.2% of the time that means 47.8%, is the training error rate


#### Linear Discriminant Analysis

Now we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. We fit the model using only the observations before 2005.

```{r echo=TRUE}
library(MASS)
train <- (Year < 2005)
lda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
lda.fit
```

We can see that 49.2% of the training observations correspond to days during which the market went down. The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in (4.24). If −0.642×Lag1−0.514×Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.

```{r echo=TRUE}
plot(lda.fit)
```

*Fig. 2 Plots of the linear discriminants*

Fig. 2 shows the  plots of the linear discriminants, obtained by computing −0.642×Lag1−0.514×Lag2 for each of the training observations. The Up and Down observations are displayed separately.


We use predict funtion for prediction in which the first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class. Finally, x contains the linear discriminants.

```{r echo=TRUE}
Smarket.2005 <- Smarket[!train, ]
Direction.2005 <- Direction[!train]
lda.pred <- predict(lda.fit, Smarket.2005)
```

```{r echo=TRUE}
lda.class <- lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
```

We can observe that LDA and logistic regression predictions are almost identical. Applying a 50% threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.

```{r echo=TRUE}
sum(lda.pred$posterior[, 1] >= .5)
sum(lda.pred$posterior[, 1] < .5)
lda.pred$posterior[1:20, 1]
lda.class[1:20]
```

We notice that the posterior probability output by the model corresponds to  the probability that the market will decrease.

#### Quadratic Discriminant Analysis

We will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. 

```{r echo= TRUE}
qda.fit <- qda(Direction~Lag1 + Lag2, data = Smarket, subset = train)
qda.fit
```

The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. For prediction-

```{r echo=TRUE}
qda.class <- predict(qda.fit, Smarket.2005)$class
table(qda.class, Direction.2005)
mean(qda.class == Direction.2005)
```

We observe that QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.

#### Naive Bayes

Next, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library.

```{r echo=TRUE}
library(e1071)
nb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
nb.fit
```

The output contains the estimated mean and standard deviation for each variable in each class.

```{r echo=TRUE}
mean(Lag1[train][Direction[train] == "Down"])
sd(Lag1[train][Direction[train] == "Down"])
```
The mean for Lag1 is 0.0428 for Direction=Down, and the standard deviation is 1.23.

```{r echo=TRUE}
nb.class <- predict(nb.fit, Smarket.2005)
table(nb.class, Direction.2005)
mean(nb.class == Direction.2005)
```

Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA.

#### K-Nearest Neighbors

We will now perform KNN using the knn() function, which is part of the class library. 

```{r echo=TRUE}
library(class)
train.X <- cbind(Lag1, Lag2)[train, ]
test.X <- cbind(Lag1, Lag2)[!train, ]
train.Direction <- Direction[train]
```


Now the knn() function can be used to predict the market’s movement for the dates in 2005. We set a random seed before we apply knn() because if several observations are tied as nearest neighbors, then R will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.

```{r echo=TRUE}
set.seed(1)
knn.pred <- knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.2005)
(83 + 43) / 252
```

We can see that the results using K = 1 are not very good, since only 50% of the observations are correctly predicted so we repeat the analysis using K = 3.

```{r echo=TRUE}
knn.pred <- knn(train.X, test.X, train.Direction, k = 3)
table(knn.pred, Direction.2005)
mean(knn.pred == Direction.2005)
```

The results have improved slightly. But increasing K further turns out to provide no further improvements.

## Conclusion

We performed logistic regression, linear discriminant analysis, naive bayes, quadratic discriminant analysis and k nearest neighbors on Smarket dataset in ISLR2 library. In the analysis we have found that QDA provides the best results among all methods that we have examined.



